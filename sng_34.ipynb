{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m37\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    text = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m10\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return parse_array(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m7\u001b[0m, in \u001b[1;35mparse_array\u001b[0m\n    return np.array(ast.literal_eval(s))\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    w_side = int((window_size - 1) / 2)\n",
    "    train = []\n",
    "    for i in range(w_side, len(text) - w_side+1):\n",
    "        center = text[i]\n",
    "        for cxtWord in text[i - w_side:i] + text[i + 1:i+w_side + 2]:\n",
    "            train.append([center, cxtWord, 1])\n",
    "            for neg in range(ns_rate):\n",
    "                r = np.random.randint(vocab_size)\n",
    "                train.append([center, r, 0])\n",
    "    return train\n",
    "\n",
    "\n",
    "text = read_array()\n",
    "window_size = int(sys.stdin.readline().strip())\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "ns_rate = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
    "\n",
    "write_array(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m38\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    center_embeddings = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m10\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return parse_array(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m7\u001b[0m, in \u001b[1;35mparse_array\u001b[0m\n    return np.array(ast.literal_eval(s))\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_word - int - identifier of center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    sigm = lambda x: 1 / (1 + np.exp(-x))\n",
    "    center_word_emb = center_embeddings[center_word]\n",
    "    context_word_emb = context_embeddings[context_word]\n",
    "    prob = sigm(center_word_emb @ context_word_emb)\n",
    "    err = prob - label\n",
    "    w_grad = err * context_word_emb\n",
    "    d_grad = err * center_word_emb\n",
    "    \n",
    "    center_embeddings[center_word] -= learning_rate * w_grad\n",
    "    context_embeddings[context_word] -= learning_rate * d_grad\n",
    "    return center_embeddings, context_embeddings\n",
    "    \n",
    "\n",
    "center_embeddings = read_array()\n",
    "context_embeddings = read_array()\n",
    "center_word = int(sys.stdin.readline().strip())\n",
    "context_word = int(sys.stdin.readline().strip())\n",
    "label = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_w2v_weights(center_embeddings, context_embeddings,\n",
    "                   center_word, context_word, label, learning_rate)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_list():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "    token2subwords - list of lists of int - i-th sublist contains list of identifiers of n-grams for token #i (list of subword units)\n",
    "\n",
    "    returns list of training samples (CenterSubwords, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    w_side = (window_size - 1) // 2\n",
    "    train = []\n",
    "    text = text.tolist()\n",
    "    for i in range(len(text)):\n",
    "        center = text[i]\n",
    "\n",
    "        left_idx = max(0, i - w_side)\n",
    "        left = text[left_idx:i]\n",
    "        right_idx = min(len(text), i+w_side+1)\n",
    "        right = text[i+1:right_idx]\n",
    "        \n",
    "        ctx = left + right\n",
    "        #print(len(ctx))\n",
    "        for cxtWord in ctx:\n",
    "            v = [center] + token2subwords[center]\n",
    "            train.append((v, cxtWord, 1))\n",
    "            for neg in range(ns_rate):\n",
    "                r = np.random.randint(vocab_size)\n",
    "                train.append((v, r, 0))\n",
    "    return train\n",
    "\n",
    "\n",
    "text = read_array()\n",
    "window_size = int(sys.stdin.readline().strip())\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "ns_rate = int(sys.stdin.readline().strip())\n",
    "token2subwords = read_list()\n",
    "\n",
    "result = generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords)\n",
    "\n",
    "print(repr(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_ft_weights(center_embeddings, context_embeddings, center_subwords, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_subwords - list of ints - list of identifiers of n-grams contained in center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    sigm = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    center_word_emb = np.zeros((1, center_embeddings.shape[1]))\n",
    "    n = len(center_subwords)\n",
    "    for w in center_subwords:\n",
    "        center_word_emb += center_embeddings[w]\n",
    "    center_word_emb /= n\n",
    "\n",
    "    context_word_emb = context_embeddings[context_word]\n",
    "    prob = sigm(center_word_emb @ context_word_emb)\n",
    "    err = prob - label\n",
    "    w_grad = err * context_word_emb / n\n",
    "    \n",
    "    for w in center_subwords:\n",
    "        center_embeddings[w] -= learning_rate * w_grad\n",
    "    d_grad = err * center_word_emb\n",
    "\n",
    "    context_embeddings[context_word] -= learning_rate * d_grad.squeeze()\n",
    "    return center_embeddings, context_embeddings\n",
    "\n",
    "center_embeddings = read_array()\n",
    "context_embeddings = read_array()\n",
    "center_subwords = read_array()\n",
    "context_word = int(sys.stdin.readline().strip())\n",
    "label = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_ft_weights(center_embeddings, context_embeddings,\n",
    "                  center_subwords, context_word, label, learning_rate)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-11-fae177d0d2b3>\"\u001b[0m, line \u001b[1;32m24\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    text = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-11-fae177d0d2b3>\"\u001b[0m, line \u001b[1;32m8\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return ast.literal_eval(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_coocurrence_matrix(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n",
    "    vocab_size - int - size of vocabulary\n",
    "    returns scipy.sparse.dok_matrix\n",
    "    \"\"\"\n",
    "    dok = scipy.sparse.dok_matrix((vocab_size, vocab_size))\n",
    "    n = len(texts)\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if i == j:\n",
    "                continue\n",
    "            for text in texts:\n",
    "                if i in text and j in text:\n",
    "                    dok[i, j] += 1\n",
    "    return dok\n",
    "\n",
    "text = read_array()\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = generate_coocurrence_matrix(text, vocab_size)\n",
    "\n",
    "write_array(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_glove_weights(x, w, d, alpha, max_x, learning_rate):\n",
    "    \"\"\"\n",
    "    x - square integer matrix VocabSize x VocabSize - coocurrence matrix\n",
    "    w - VocabSize x EmbSize - first word vectors\n",
    "    d - VocabSize x EmbSize - second word vectors\n",
    "    alpha - float - power in weight smoothing function f\n",
    "    max_x - int - maximum coocurrence count in weight smoothing function f\n",
    "    learning_rate - positive float - size of gradient step\n",
    "    \"\"\"\n",
    "    f = lambda x: np.where(x <= max_x, (x / max_x) ** alpha, 1.0)\n",
    "    fx = f(x)\n",
    "    logx = np.log1p(x)\n",
    "    wd = w @ d.T\n",
    "    \n",
    "    err = fx * (logx - wd)\n",
    "    dw = (-2 * err) @ d\n",
    "    dd = (-2 * err).T @ w\n",
    "    w[:] = w - learning_rate * dw\n",
    "    d[:] = d - learning_rate * dd\n",
    "\n",
    "x = read_array()\n",
    "w = read_array()\n",
    "d = read_array()\n",
    "alpha = float(sys.stdin.readline().strip())\n",
    "max_x = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_glove_weights(x, w, d, alpha, max_x, learning_rate)\n",
    "\n",
    "write_array(w)\n",
    "write_array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def get_nearest(embeddings, query_word_id, get_n):\n",
    "    \"\"\"\n",
    "    embeddings - VocabSize x EmbSize - word embeddings\n",
    "    query_word_id - integer - id of query word to find most similar to\n",
    "    get_n - integer - number of most similar words to retrieve\n",
    "\n",
    "    returns list of `get_n` tuples (word_id, similarity) sorted by descending order of similarity value\n",
    "    \"\"\"\n",
    "    embbeddings /= np.linalg.norm(embeddings, ord=2, axis=0)\n",
    "    word_emb = embeddings[query_word_id]\n",
    "    distances = embeddings @ word_emb\n",
    "    top_idx = np.argsort(distances)[::-1][:get_n]\n",
    "    answer = []\n",
    "    for idx in top_idx:\n",
    "        answer.append((idx, distances[idx]))\n",
    "    return answer\n",
    "\n",
    "\n",
    "embeddings = read_array()\n",
    "query_word_id = int(sys.stdin.readline().strip())\n",
    "get_n = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = get_nearest(embeddings, query_word_id, get_n)\n",
    "\n",
    "write_array(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.4924379807931911, 0.4372909146117835, 0.3786715543730712, 0.08765497537657796, 0.6756358041845987, 0.21918880613708924, 0.340282751187035, 0.7564088132708989], [0.44017230951695874, 0.1521570621825502, 0.5269119216892, 0.13969272399772448, 0.8905714313092886, 0.6855990837043952, 0.7329403885550506, 0.6967890760190271], [0.08209802635148533, 0.1572554699218176, 0.32713724494890817, 0.3282258214949071, 0.18745515454533546, 0.6684444254856369, 0.3821374793787291, 0.14405318337384954], [0.4685806535328395, 0.6823014264406443, 0.45242992021735506, 0.13462511393109822, 0.37643757313276727, 0.33711083265770425, 0.953787573113227, 0.9375271756567445], [0.39126114202292495, 0.4240577502243138, 0.45596962948649294, 0.8405515982747295, -0.0035639217032158305, 0.08033917718260508, 0.7064913228196669, 0.6149593432967865], [0.47541554931084073, 0.6053107399554578, 0.7731362614524417, 0.43745637956185424, 0.06569482940157491, 0.3140376497070242, 0.17562601577455203, 0.06255425666405454], [0.4623567291387256, 0.6292685831821194, 0.35307170833144663, 0.48720150493272496, 0.27648124218553927, 0.40031870928193414, 0.02599896525632328, 0.6563730701085458], [0.7006508390687906, 0.9514452459838603, 0.2451080863945575, 0.5123482317902458, 0.0073772504296028, 0.04790725982848376, 0.7551912304218487, 0.05519748500067578], [0.7949394482871045, 0.6139101679060078, 0.9636668795699912, 0.20793187470932273, 0.5691843546570802, 0.64175464297284, 0.5316441135537809, 0.6480149378052954], [0.9602567071597025, 0.6144154154415676, 0.6354348353858741, 0.8176817682689422, 0.4365203872332607, 0.6018591595887026, 0.24588600970700025, 0.8417956150068155], [0.27828446555932296, 0.4807798024399753, 0.28161693377865327, 0.5774428203991377, 0.18228473976288317, 0.16857605985792, 0.4559900707914125, 0.9441427490521547], [0.7414879823479924, 0.8647281941270776, 0.35393371610868524, 0.2804504748200687, 0.4701667907804358, 0.8861670917859702, 0.6892073557636252, 0.1330433396753354], [0.20061898730429373, 0.13089938905380893, 0.2180695062782767, 0.16685666837275925, 0.7485275811921536, 0.0009222052073699638, 0.8764827495937608, 0.7118160874636742], [0.4102880272659505, 0.6911504017206197, 0.674527579437148, 0.9585605434693513, 0.9184313869364793, 0.13068786160259238, 0.7817381372985575, 0.11406856860587689], [0.9637996777388024, 0.9096498590978186, 0.09897429193559693, 0.6229947983032628, 0.6103840977039744, 0.07802796500873965, 0.27955682331764764, 0.7458814824074171], [0.7543834769837153, 0.6104516650024087, 0.350083552653517, 0.23679074594453386, 0.26123401278746905, 0.21552344000610668, 0.16486681789984003, 0.5376355458748053], [0.970347909471565, 0.531785083567912, 0.6761950893288675, 0.84233111602816, 0.7717486528345849, 0.19409526576427183, 0.6650316349624878, 0.9772140892554211], [0.2447519981151074, 0.11727034778098522, 0.4645834487138182, 0.0953191957887265, 0.18973178771818744, 0.7330847257058634, 0.1697697394333707, 0.08721362050017656], [0.5991518757961101, 0.15871166110849755, 0.5804090119102789, 0.27786186874031715, 0.08935690910963434, 0.30450146770449615, 0.6783458480620492, 0.17629782226896706], [0.4724575749096397, 0.7272444260638546, 0.45727256576998476, 0.13904986666789265, 0.33443609088716, 0.7534586544539846, 0.6994487600617424, 0.06700176145670023], [0.48069977927688057, 0.28453027574930045, 0.6026090887574915, 0.5395475284054398, 0.43738833425061685, 0.44290239322523606, 0.9159159974831983, 0.5758008552741829]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [[0.4924379807931911, 0.4372909146117835, 0.3786715543730712, 0.08765497537657796, 0.6756358041845987, 0.21918880613708924, 0.340282751187035, 0.7564088132708989], [0.44017230951695874, 0.1521570621825502, 0.5269119216892, 0.13969272399772448, 0.8905714313092886, 0.6855990837043952, 0.7329403885550506, 0.6967890760190271], [0.08209802635148533, 0.1572554699218176, 0.32713724494890817, 0.3282258214949071, 0.18745515454533546, 0.6684444254856369, 0.3821374793787291, 0.14405318337384954], [0.4685806535328395, 0.6823014264406443, 0.45242992021735506, 0.13462511393109822, 0.37643757313276727, 0.33711083265770425, 0.953787573113227, 0.9375271756567445], [0.3747200281624562, 0.2957023888332825, 0.4268682014731547, 0.7845482742542396, -0.20545128867001977, -0.10572156027398755, 0.556371523841753, 0.5072352412236689], [0.458874435450372, 0.4769553785644265, 0.7440348334391035, 0.3814530555413644, -0.13619253756522903, 0.12797691225043156, 0.025506216796638226, -0.045169845409062964], [0.4623567291387256, 0.6292685831821194, 0.35307170833144663, 0.48720150493272496, 0.27648124218553927, 0.40031870928193414, 0.02599896525632328, 0.6563730701085458], [0.7006508390687906, 0.9514452459838603, 0.2451080863945575, 0.5123482317902458, 0.0073772504296028, 0.04790725982848376, 0.7551912304218487, 0.05519748500067578], [0.7949394482871045, 0.6139101679060078, 0.9636668795699912, 0.20793187470932273, 0.5691843546570802, 0.64175464297284, 0.5316441135537809, 0.6480149378052954], [0.9602567071597025, 0.6144154154415676, 0.6354348353858741, 0.8176817682689422, 0.4365203872332607, 0.6018591595887026, 0.24588600970700025, 0.8417956150068155], [0.27828446555932296, 0.4807798024399753, 0.28161693377865327, 0.5774428203991377, 0.18228473976288317, 0.16857605985792, 0.4559900707914125, 0.9441427490521547], [0.7249468684875237, 0.7363728327360463, 0.324832288095347, 0.22444715079957883, 0.26827942381363185, 0.7001063543293775, 0.5390875567857114, 0.02531923760221791], [0.20061898730429373, 0.13089938905380893, 0.2180695062782767, 0.16685666837275925, 0.7485275811921536, 0.0009222052073699638, 0.8764827495937608, 0.7118160874636742], [0.4102880272659505, 0.6911504017206197, 0.674527579437148, 0.9585605434693513, 0.9184313869364793, 0.13068786160259238, 0.7817381372985575, 0.11406856860587689], [0.9637996777388024, 0.9096498590978186, 0.09897429193559693, 0.6229947983032628, 0.6103840977039744, 0.07802796500873965, 0.27955682331764764, 0.7458814824074171], [0.7543834769837153, 0.6104516650024087, 0.350083552653517, 0.23679074594453386, 0.26123401278746905, 0.21552344000610668, 0.16486681789984003, 0.5376355458748053], [0.970347909471565, 0.531785083567912, 0.6761950893288675, 0.84233111602816, 0.7717486528345849, 0.19409526576427183, 0.6650316349624878, 0.9772140892554211], [0.2447519981151074, 0.11727034778098522, 0.4645834487138182, 0.0953191957887265, 0.18973178771818744, 0.7330847257058634, 0.1697697394333707, 0.08721362050017656], [0.5991518757961101, 0.15871166110849755, 0.5804090119102789, 0.27786186874031715, 0.08935690910963434, 0.30450146770449615, 0.6783458480620492, 0.17629782226896706], [0.4724575749096397, 0.7272444260638546, 0.45727256576998476, 0.13904986666789265, 0.33443609088716, 0.7534586544539846, 0.6994487600617424, 0.06700176145670023], [0.48069977927688057, 0.28453027574930045, 0.6026090887574915, 0.5395475284054398, 0.43738833425061685, 0.44290239322523606, 0.9159159974831983, 0.5758008552741829]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~np.isclose(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
