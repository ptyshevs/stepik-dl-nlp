{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m37\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    text = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m10\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return parse_array(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"<ipython-input-2-e8a92b4927b4>\"\u001b[0m, line \u001b[1;32m7\u001b[0m, in \u001b[1;35mparse_array\u001b[0m\n    return np.array(ast.literal_eval(s))\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    w_side = int((window_size - 1) / 2)\n",
    "    train = []\n",
    "    for i in range(w_side, len(text) - w_side+1):\n",
    "        center = text[i]\n",
    "        for cxtWord in text[i - w_side:i] + text[i + 1:i+w_side + 2]:\n",
    "            train.append([center, cxtWord, 1])\n",
    "            for neg in range(ns_rate):\n",
    "                r = np.random.randint(vocab_size)\n",
    "                train.append([center, r, 0])\n",
    "    return train\n",
    "\n",
    "\n",
    "text = read_array()\n",
    "window_size = int(sys.stdin.readline().strip())\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "ns_rate = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
    "\n",
    "write_array(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m38\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    center_embeddings = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m10\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return parse_array(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"<ipython-input-3-bdf853a09c47>\"\u001b[0m, line \u001b[1;32m7\u001b[0m, in \u001b[1;35mparse_array\u001b[0m\n    return np.array(ast.literal_eval(s))\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_word - int - identifier of center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    sigm = lambda x: 1 / (1 + np.exp(-x))\n",
    "    center_word_emb = center_embeddings[center_word]\n",
    "    context_word_emb = context_embeddings[context_word]\n",
    "    prob = sigm(center_word_emb @ context_word_emb)\n",
    "    err = prob - label\n",
    "    w_grad = err * context_word_emb\n",
    "    d_grad = err * center_word_emb\n",
    "    \n",
    "    center_embeddings[center_word] -= learning_rate * w_grad\n",
    "    context_embeddings[context_word] -= learning_rate * d_grad\n",
    "    return center_embeddings, context_embeddings\n",
    "    \n",
    "\n",
    "center_embeddings = read_array()\n",
    "context_embeddings = read_array()\n",
    "center_word = int(sys.stdin.readline().strip())\n",
    "context_word = int(sys.stdin.readline().strip())\n",
    "label = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_w2v_weights(center_embeddings, context_embeddings,\n",
    "                   center_word, context_word, label, learning_rate)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_list():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "    token2subwords - list of lists of int - i-th sublist contains list of identifiers of n-grams for token #i (list of subword units)\n",
    "\n",
    "    returns list of training samples (CenterSubwords, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    w_side = (window_size - 1) // 2\n",
    "    train = []\n",
    "    text = text.tolist()\n",
    "    for i in range(len(text)):\n",
    "        center = text[i]\n",
    "\n",
    "        left_idx = max(0, i - w_side)\n",
    "        left = text[left_idx:i]\n",
    "        right_idx = min(len(text), i+w_side+1)\n",
    "        right = text[i+1:right_idx]\n",
    "        \n",
    "        ctx = left + right\n",
    "        #print(len(ctx))\n",
    "        for cxtWord in ctx:\n",
    "            v = [center] + token2subwords[center]\n",
    "            train.append((v, cxtWord, 1))\n",
    "            for neg in range(ns_rate):\n",
    "                r = np.random.randint(vocab_size)\n",
    "                train.append((v, r, 0))\n",
    "    return train\n",
    "\n",
    "\n",
    "text = read_array()\n",
    "window_size = int(sys.stdin.readline().strip())\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "ns_rate = int(sys.stdin.readline().strip())\n",
    "token2subwords = read_list()\n",
    "\n",
    "result = generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords)\n",
    "\n",
    "print(repr(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_ft_weights(center_embeddings, context_embeddings, center_subwords, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_subwords - list of ints - list of identifiers of n-grams contained in center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    sigm = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    center_word_emb = np.zeros((1, center_embeddings.shape[1]))\n",
    "    n = len(center_subwords)\n",
    "    for w in center_subwords:\n",
    "        center_word_emb += center_embeddings[w]\n",
    "    center_word_emb /= n\n",
    "\n",
    "    context_word_emb = context_embeddings[context_word]\n",
    "    prob = sigm(center_word_emb @ context_word_emb)\n",
    "    err = prob - label\n",
    "    w_grad = err * context_word_emb / n\n",
    "    \n",
    "    for w in center_subwords:\n",
    "        center_embeddings[w] -= learning_rate * w_grad\n",
    "    d_grad = err * center_word_emb\n",
    "\n",
    "    context_embeddings[context_word] -= learning_rate * d_grad.squeeze()\n",
    "    return center_embeddings, context_embeddings\n",
    "\n",
    "center_embeddings = read_array()\n",
    "context_embeddings = read_array()\n",
    "center_subwords = read_array()\n",
    "context_word = int(sys.stdin.readline().strip())\n",
    "label = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_ft_weights(center_embeddings, context_embeddings,\n",
    "                  center_subwords, context_word, label, learning_rate)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ptyshevs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-11-fae177d0d2b3>\"\u001b[0m, line \u001b[1;32m24\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    text = read_array()\n",
      "  File \u001b[1;32m\"<ipython-input-11-fae177d0d2b3>\"\u001b[0m, line \u001b[1;32m8\u001b[0m, in \u001b[1;35mread_array\u001b[0m\n    return ast.literal_eval(sys.stdin.readline())\n",
      "  File \u001b[1;32m\"/usr/lib/python3.6/ast.py\"\u001b[0m, line \u001b[1;32m48\u001b[0m, in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_coocurrence_matrix(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n",
    "    vocab_size - int - size of vocabulary\n",
    "    returns scipy.sparse.dok_matrix\n",
    "    \"\"\"\n",
    "    dok = scipy.sparse.dok_matrix((vocab_size, vocab_size))\n",
    "    n = len(texts)\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if i == j:\n",
    "                continue\n",
    "            for text in texts:\n",
    "                if i in text and j in text:\n",
    "                    dok[i, j] += 1\n",
    "    return dok\n",
    "\n",
    "text = read_array()\n",
    "vocab_size = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = generate_coocurrence_matrix(text, vocab_size)\n",
    "\n",
    "write_array(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_glove_weights(x, w, d, alpha, max_x, learning_rate):\n",
    "    \"\"\"\n",
    "    x - square integer matrix VocabSize x VocabSize - coocurrence matrix\n",
    "    w - VocabSize x EmbSize - first word vectors\n",
    "    d - VocabSize x EmbSize - second word vectors\n",
    "    alpha - float - power in weight smoothing function f\n",
    "    max_x - int - maximum coocurrence count in weight smoothing function f\n",
    "    learning_rate - positive float - size of gradient step\n",
    "    \"\"\"\n",
    "    f = lambda x: np.where(x <= max_x, (x / max_x) ** alpha, 1.0)\n",
    "    fx = f(x)\n",
    "    logx = np.log1p(x)\n",
    "    wd = w @ d.T\n",
    "    \n",
    "    err = fx * (logx - wd)\n",
    "    dw = (-2 * err) @ d\n",
    "    dd = (-2 * err).T @ w\n",
    "    w[:] = w - learning_rate * dw\n",
    "    d[:] = d - learning_rate * dd\n",
    "\n",
    "x = read_array()\n",
    "w = read_array()\n",
    "d = read_array()\n",
    "alpha = float(sys.stdin.readline().strip())\n",
    "max_x = int(sys.stdin.readline().strip())\n",
    "learning_rate = float(sys.stdin.readline().strip())\n",
    "\n",
    "update_glove_weights(x, w, d, alpha, max_x, learning_rate)\n",
    "\n",
    "write_array(w)\n",
    "write_array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def get_nearest(embeddings, query_word_id, get_n):\n",
    "    \"\"\"\n",
    "    embeddings - VocabSize x EmbSize - word embeddings\n",
    "    query_word_id - integer - id of query word to find most similar to\n",
    "    get_n - integer - number of most similar words to retrieve\n",
    "\n",
    "    returns list of `get_n` tuples (word_id, similarity) sorted by descending order of similarity value\n",
    "    \"\"\"\n",
    "    embbeddings /= np.linalg.norm(embeddings, ord=2, axis=0)\n",
    "    word_emb = embeddings[query_word_id]\n",
    "    distances = embeddings @ word_emb\n",
    "    top_idx = np.argsort(distances)[::-1][:get_n]\n",
    "    answer = []\n",
    "    for idx in top_idx:\n",
    "        answer.append((idx, distances[idx]))\n",
    "    return answer\n",
    "\n",
    "\n",
    "embeddings = read_array()\n",
    "query_word_id = int(sys.stdin.readline().strip())\n",
    "get_n = int(sys.stdin.readline().strip())\n",
    "\n",
    "result = get_nearest(embeddings, query_word_id, get_n)\n",
    "\n",
    "write_array(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41088"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 * 64 * 128 + 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
